---
title: "HW 9"
author: "SDS348 Fall 2019"
date: ""
output:
  html_document: default
  pdf_document: default
---

```{r global_options, include=FALSE}
#LEAVE THIS CHUNK ALONE!
library(knitr)
opts_chunk$set(fig.align="center", fig.height=5, message=FALSE, warning=FALSE, fig.width=8, tidy.opts=list(width.cutoff=60),tidy=TRUE)

class_diag<-function(probs,truth){
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  #CALCULATE EXACT AUC
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}
```

## 

**This homework is due on Nov 10, 2019 at 11:59pm. Please submit as a pdf file on Canvas.**

*For all questions, include the R commands/functions that you used to find your answer. Answers without supporting code will not receive credit.*

> ### How to submit this assignment
> All homework assignments will be completed using R Markdown. These `.Rmd` files consist of text/syntax (formatted using Markdown) alongside embedded R code. 
> When you have completed the assignment (by adding R code inside codeblocks and supporting text outside of the codeblocks), create your document as follows:

> - Click the "Knit" button (above) to create an .html file
> - Open the html file in your internet browser to view
> - Go to `File > Print` and print your .html file to a .pdf
> - (or knit to PDF)
> - Upload the .pdf file to Canvas


---

### Question 1 

Back to Pokemon! (Sorry if that's not your thing...) There is a somewhat famous DataCamp module for predicting what makes a pokemon legendary and I wanted to put my own spin on it. 

1a. (1 pts) First, run the following code to read in the data and drop the unnecessary variables. With the resulting dataset `poke`, how many are Legendary? How many are not?

```{R}
library(tidyverse)
poke<-read.csv("https://gist.githubusercontent.com/armgilles/194bcff35001e7eb53a2a8b441e8b2c6/raw/92200bc0a673d5ce2110aaad4544ed6c4010f687/pokemon.csv",row.names = "Name")
poke<-poke%>%dplyr::select(-`X.`,-Total)

poke%>%group_by(Legendary)%>%summarize(n=n())
```

**

1b. (2 pts) Predict Legendary from all of the remaining variables (recall the shortcut for this: `Legendary~.`) using a logistic regression. You don't need to convert it: R is smart enough to do this for you. Generate predicted probabilities for your original observations and save them as an object called `prob` in your environment (don't save them to the `poke` dataset). Use them to compute classification diagnostics with the `class_diag()` function from class (or the equivalent: it is declared in the preamble above so you should be able to use it in any subsequent code chunk). How well is the model performing per AUC? Provide a confusion matrix too and provide brief interpretation of what you see. 

```{R}
fit<-glm(Legendary~.,data=poke,family="binomial")
prob<-predict(fit,type="response")
class_diag(prob,poke$Legendary)

table(predictions=ifelse(prob>.5,1,0),truth=poke$Legendary)%>%addmargins
```

**

1c. (3 pts) Now perform 10-fold cross validation with this model. Summarize the results by reporting average classification diagnostics (e.g., from `class_diags()`) across the ten folds (you might get a `NaN` for ppv, which is fine). Do you see a substantial decrease in AUC when predicting out of sample? (Do you this model shows signs of overfitting?)

```{R}
set.seed(1234)
k=10

data1<-poke[sample(nrow(poke)),]
folds<-cut(seq(1:nrow(poke)),breaks=k,labels=F)

diags<-NULL
for(i in 1:k){
  train<-data1[folds!=i,]
  test<-data1[folds==i,]
  truth<-test$Legendary
  
  fit<-glm(Legendary~.,data=train,family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  
  diags<-rbind(diags,class_diag(probs,truth))
}

apply(diags,2,mean)
```

**

1d. (3 pts) OK, now perform a LASSO regression on the same model (i.e., predicting Legendary from all predictor variables). You will need to have your predictors in a matrix and your response variable in a separate matrix. A good idea would be to use model.matrix(fit) from above to get the predictor-variable matrix (do not include the (intercept) term; i.e., drop the first column). To get the response, just do `as.matrix(poke$Legendary)`. Perform cross-validation via `cv.glmnet` to select the regularization parameter lambda and pick `lambda.1se` to regularize. Which coefficient estimates are non-zero? Report classification diagnostics. Also, provide a confusion matrix and talk about what you see. How does this compare with the full model from 1b?

```{R}
library(glmnet)

fit<-glm(Legendary~.,data=poke,family="binomial")
x<-model.matrix(fit)
x<-x[,-1]
x<-scale(x)

poke2<-poke%>%mutate(Legendary=ifelse(Legendary=="True",1,0))
y<-as.matrix(poke2$Legendary)
cv<-cv.glmnet(x,y,family="binomial")

lasso1<-glmnet(x,y,lambda=cv$lambda.1se,family="binomial")
coef(lasso1)

poke2$T1Flying<-ifelse(poke2$Type.1=="Flying",1,0)
poke2$T1Psychic<-ifelse(poke2$Type.1=="Psychic",1,0)

fit<-glm(Legendary~T1Flying+T1Psychic+HP+Attack+Defense+Sp..Atk+Sp..Def+Speed+Generation,data=poke2,family="binomial")
prob<-predict(fit,type="response")

class_diag(prob,poke2$Legendary)

table(predictions=ifelse(prob>.5,1,0),truth=poke2$Legendary)%>%addmargins
```

**

1e. (2 pts) Re-run your 10-fold CV from above (1c), but this time use only the predictor variables which had non-zero LASSO coefficient estimates (like you did in lab). Note that you will need to use dummies/indicator variables for the significant types, so you might grab the corresponding vars from the model.matrix(fit) you used above. How does this model compare to the full model in terms of out-of-sample prediction? 

```{R}
set.seed(1234)
k=10

# your code here
data1<-poke2[sample(nrow(poke)),]
folds<-cut(seq(1:nrow(poke)),breaks=k,labels=F)

diags<-NULL
for(i in 1:k){
  train<-data1[folds!=i,]
  test<-data1[folds==i,]
  truth<-test$Legendary
  
  fit<-glm(Legendary~T1Flying+T1Psychic+HP+Attack+Defense+Sp..Atk+Sp..Def+Speed+Generation,data=train,family="binomial")
  probs<-predict(fit,newdata = test,type="response")
  
  diags<-rbind(diags,class_diag(probs,truth))
}

apply(diags,2,mean)
```

**

1f. (1 pts) Let's do something *fancy-sounding*. We will use a random forest classifier to predict Legendary status from our predictor variables. In brief, this technique uses decision trees fitted (or "grown") on multiple bootstrapped samples of the data (i.e., with replacement), using bootstrapped samples of the predictor variables to predict (so not every model has all of the predictors: interesting twist!). The predictions (legendary/not) from each such tree are then averaged ("bagged" or bootstrap-aggregated); the proportion of trees predicting each pokemon as Legendary can be gotten and used as a probability for diagnostic purposes. Just like in CV, averaging performance (of many decision trees) across many (bootstrapped) samples reduces overfitting and makes our predictions more robust to noise. We will use these proportions to compare classification performance with our logistic regression (using the variables selected by lasso regularization, a technique also designed to curtail overfitting). Note that we are using the random forest function right out-of-the-box and better performance could be achieved if we tuned certain argumentes (e.g., ntree, mtry).

Because the technique is based on fitting a model to repeated bootstrapped samples and aggregating across predictions, we don't expect that using k-fold CV will show much difference. But let's see! Run the following code to (1) fit the random forest and produce the classification diagnostics and (2) perform 10-fold CV on the random forest model (to show it doesn't really change). 

Compare the classification performance of the logistic regression using the variables lasso selected (i.e., the CV performance) with the classification performance of the random forest. Is there much difference?


```{R}
#install.packages("randomForest")
library(randomForest) 
fit_rf=randomForest(Legendary~.,data=poke)

class_diag(fit_rf$votes[,2],poke$Legendary)


######### CV
set.seed(1234)
k=10

data1<-poke[sample(nrow(poke)),]
folds<-cut(seq(1:nrow(poke)),breaks=k,labels=F)

diags<-NULL
for(i in 1:k){
  train<-data1[folds!=i,]
  test<-data1[folds==i,]
  truth<-test$Legendary
  
  fit<-randomForest(Legendary~.,data=train)
  probs<-predict(fit,newdata = test,type="prob")[,2]

diags<-rbind(diags,class_diag(probs,truth))
}

diags%>%summarize_all(mean)
```

**

2a. (2 pts) Below, you are given 6 malignant patients and 6 benign patients. The vectors contain their predicted probabilities (i.e., the probability of malignancy from some model). If you compare every malignant patient with every benign patient, how many times does a malignant patient have a higher predicted probability than a benign patient? What proportion of all the comparisons is that? You can easily do this by hand, but you might try to find a way to use `expand.grid()`, `outer()`, or even a loop to calculate this in R (use ?functionname to read about these functions).

```{R}
malig<-c(.49, .36, .57, .53, .61, .66)
benign<-c(.41, .22, .26, .56, .31 ,.39)

sum(outer(malig,benign,"-")>0)
mean(outer(malig,benign,"-")>0)
```

**

2b (1 pts) Now, treat the predicted probabilities as the response variable and perform an Wilcoxon/Mann-Whitney U test in R using `wilcox.test(group1, group2)` comparing the distribution of predicted probabilities for both groups (malig and benign). What does your W/U statistic equal?

```{R}
wilcox.test(malig, benign)
```
**


2c (2 pts) Tidy this data by creating a dataframe and putting all predicted probabilities into one column and malignant/benign labels in another (you should end up with twelve rows, one for each observation). Use this data and ggplot to make a graph of the distribution of probabilities for both groups (histogram): fill by group. Leave default binwidth alone (it will look kind of like a barcode). Eyeballing and counting manually, for each benign (red) compute the number of malignants (blue) it is greater than (blue) and add them all up. This is the number of times a benign has a higher predicted probability than a malignant! In 1a you found the number of times a malignant beats a benign (i.e., has a higher predicted probability than a benign): What do those two numbers add up to?

```{R}
dat<-cbind(malig,benign)%>%as.data.frame()
dat<-dat%>%pivot_longer(cols=c(1,2),names_to="Type",values_to="Prob")

dat%>%ggplot(aes(Prob))+geom_histogram(aes(fill=Type))
```

**


2d (2 pts) Set the cutoff/threshold at .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7 and for for each cutoff, compute the true-positive rate (TPR) and the false-postive rate (FPR). You may do this manually, but I encourage you to try to figure out a way to do it in R (e.g., using `expand.grid` and `dplyr` functions). Save the TPR and FPR for each cut-off. Then make a plot of the TPR (y-axis) against the FPR (x-axis) using geom_path.

```{R}
cutoffs<-c(.2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7)

c<-expand.grid(cutoff=cutoffs,prob=dat[dat$Type=="malig",]$Prob)
d<-expand.grid(cutoff=cutoffs,prob=dat[dat$Type=="benign",]$Prob)

d<-d%>%mutate(eval=prob>cutoff)%>%group_by(cutoff)%>%summarize(FPR=sum(eval)/6)
c<-c%>%mutate(eval=prob>cutoff)%>%group_by(cutoff)%>%summarize(TPR=sum(eval)/6)
posrates<-left_join(c,d)
posrates%>%ggplot(aes(FPR,TPR))+geom_path()
```


2e (1 pt) Use the `class_diags()` function to calculate the AUC (and other diagnostics on this data). Where in this assignment have you seen that AUC number before? (If you haven't seen that number before, go back and redo 2a and make sure you are answering both questions.)

```{R}
dat<-dat%>%mutate(binaryType=ifelse(Type=="malig",1,0))
class_diag(dat$Prob,dat$binaryType)
```

**

```{R, echo=F}
## DO NOT DELETE THIS CHUNK!
sessionInfo()
Sys.time()
Sys.info()
```